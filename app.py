# --- Imports ---
import streamlit as st # Biblioteca do Streamlit
import os
from dotenv import load_dotenv

# LLM (Gemini)
from langchain_google_genai import ChatGoogleGenerativeAI
# Embeddings (Local/Gratuita)
from langchain_community.embeddings import HuggingFaceEmbeddings
# Vector Store (Local/Gratuita)
from langchain_community.vectorstores import FAISS
# Cadeias (Chains)
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# Carrega as vari√°veis de ambiente (sua GOOGLE_API_KEY)
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

# --- Constantes ---
# O NOME DA PASTA QUE VOC√ä FEZ UPLOAD PARA O GITHUB
FAISS_INDEX_PATH = "faiss_index_projeto" 

# --- Fun√ß√µes Cacheadas (A M√°gica do Streamlit) ---

# O @st.cache_resource "guarda" o modelo na mem√≥ria do Streamlit.
# Isso garante que s√≥ vamos baixar/carregar o modelo UMA VEZ.
@st.cache_resource
def get_embeddings_model():
    print("Carregando modelo de embedding (HuggingFace)...")
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    model_kwargs = {'device': 'cpu'}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs
    )
    print("Modelo de embedding carregado.")
    return embeddings

# Cacheia o carregamento do LLM
@st.cache_resource
def get_llm():
    print("Carregando LLM (Gemini)...")
    llm = ChatGoogleGenerativeAI(
        model="gemini-pro", 
        google_api_key=google_api_key,
        convert_system_message_to_human=True
    )
    print("LLM carregado.")
    return llm

# Cacheia o carregamento do √≠ndice FAISS (que est√° no GitHub)
@st.cache_resource
def load_faiss_index(embeddings_model):
    print("Carregando √≠ndice FAISS local...")
    # Carrega o √≠ndice da pasta (que voc√™ upou pro GitHub)
    db = FAISS.load_local(
        FAISS_INDEX_PATH, 
        embeddings_model, 
        allow_dangerous_deserialization=True # Necess√°rio para carregar do disco
    )
    print("√çndice FAISS carregado.")
    return db

# Cacheia a cria√ß√£o da cadeia RAG
@st.cache_resource
def get_rag_chain(_llm, _retriever):
    print("Criando a cadeia RAG...")
    # Template do Prompt
    prompt_template = """
    Voc√™ √© um assistente especialista. Responda a pergunta *apenas* com base no contexto.
    Contexto: {context}
    Pergunta: {input}
    Resposta:
    """
    prompt = ChatPromptTemplate.from_template(prompt_template)
    
    # Cria as duas cadeias
    document_chain = create_stuff_documents_chain(_llm, prompt)
    retrieval_chain = create_retrieval_chain(_retriever, document_chain)
    print("Cadeia RAG pronta.")
    return retrieval_chain

# --- Interface do Streamlit ---

st.set_page_config(page_title="Chat com Documentos", layout="wide")
st.title("üìÑ Chatbot com Documentos (Usando Gemini)")

# Garante que a API Key foi configurada
if not google_api_key:
    st.error("GOOGLE_API_KEY n√£o encontrada! Configure-a nos 'Secrets' do Streamlit.")
else:
    try:
        # --- Carregamento dos Modelos (via cache) ---
        embeddings = get_embeddings_model()
        llm = get_llm()
        
        # Carrega o √≠ndice FAISS e o transforma em um "buscador"
        db = load_faiss_index(embeddings)
        retriever = db.as_retriever(search_kwargs={"k": 4})
        
        # Carrega a cadeia RAG
        chain = get_rag_chain(llm, retriever)

        # --- Interface de Chat ---
        st.write("O √≠ndice est√° carregado. Fa√ßa sua pergunta sobre o documento.")
        
        # Input do usu√°rio
        user_question = st.text_input("Sua pergunta:")

        if user_question:
            # Mostra um "spinner" enquanto pensa
            with st.spinner("Pensando... (Consultando o Gemini e o √≠ndice)"):
                # Invoca a cadeia
                response = chain.invoke({"input": user_question})
                
                # Mostra a resposta
                st.subheader("Resposta:")
                st.write(response["answer"])

    except Exception as e:
        st.error(f"Ocorreu um erro ao carregar os componentes: {e}")
        st.error("Verifique se a pasta 'faiss_index_projeto' existe no seu reposit√≥rio.")